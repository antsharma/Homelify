{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJa1a8YhKLU6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# sklearn preprocessing for dealing with categorical variables\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# File system manangement\n",
        "import os\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# matplotlib and seaborn for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(\"./input/\"))"
      ],
      "metadata": {
        "id": "v2XHugy9KSP-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "7169372a-4218-4f00-d80e-cf74cdc1f5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './input/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9b039bad07f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./input/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './input/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app_train = pd.read_csv('./input/application_train.csv')\n",
        "print('Training data shape: ', app_train.shape)\n",
        "app_train.head()"
      ],
      "metadata": {
        "id": "M1m7xH7LKUk3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "83adff0e-0a1c-4ac6-b8a6-3f9def609db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './input/application_train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9f3306f5f370>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/application_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training data shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mapp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './input/application_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app_test = pd.read_csv('./input/application_test.csv')\n",
        "print('Testing data shape: ', app_test.shape)\n",
        "app_test.head()"
      ],
      "metadata": {
        "id": "vQRLgCe9KXFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train['TARGET'].value_counts()"
      ],
      "metadata": {
        "id": "gmQdBOCWKZfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train['TARGET'].astype(int).plot.hist();"
      ],
      "metadata": {
        "id": "KkC67TwjKcCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def missing_values_table(df):\n",
        "        # Total missing values\n",
        "        mis_val = df.isnull().sum()\n",
        "\n",
        "        # Percentage of missing values\n",
        "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "\n",
        "        # Make a table with the results\n",
        "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "\n",
        "        # Rename the columns\n",
        "        mis_val_table_ren_columns = mis_val_table.rename(\n",
        "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "\n",
        "        # Sort the table by percentage of missing descending\n",
        "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "        '% of Total Values', ascending=False).round(1)\n",
        "\n",
        "        # Print some summary information\n",
        "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
        "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
        "              \" columns that have missing values.\")\n",
        "\n",
        "        # Return the dataframe with missing information\n",
        "        return mis_val_table_ren_columns"
      ],
      "metadata": {
        "id": "3XWLdyRgKeuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values statistics\n",
        "missing_values = missing_values_table(app_train)\n",
        "missing_values.head(20)"
      ],
      "metadata": {
        "id": "7hN5zihXKhe-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "cd505827-d29a-4f1c-f60f-2885211d0b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'app_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4596f26113b8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Missing values statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmissing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing_values_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmissing_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'app_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of each type of column\n",
        "app_train.dtypes.value_counts()"
      ],
      "metadata": {
        "id": "wzWTJQg8Kj55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of unique classes in each object column\n",
        "app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
      ],
      "metadata": {
        "id": "dWSHQIq8KmOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a label encoder object\n",
        "le = LabelEncoder()\n",
        "le_count = 0\n",
        "\n",
        "# Iterate through the columns\n",
        "for col in app_train:\n",
        "    if app_train[col].dtype == 'object':\n",
        "        # If 2 or fewer unique categories\n",
        "        if len(list(app_train[col].unique())) <= 2:\n",
        "            # Train on the training data\n",
        "            le.fit(app_train[col])\n",
        "            # Transform both training and testing data\n",
        "            app_train[col] = le.transform(app_train[col])\n",
        "            app_test[col] = le.transform(app_test[col])\n",
        "\n",
        "            # Keep track of how many columns were label encoded\n",
        "            le_count += 1\n",
        "\n",
        "print('%d columns were label encoded.' % le_count)"
      ],
      "metadata": {
        "id": "WnqVk3GfKo8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding of categorical variables\n",
        "app_train = pd.get_dummies(app_train)\n",
        "app_test = pd.get_dummies(app_test)\n",
        "\n",
        "print('Training Features shape: ', app_train.shape)\n",
        "print('Testing Features shape: ', app_test.shape)"
      ],
      "metadata": {
        "id": "3K-iWLqQKv0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = app_train['TARGET']\n",
        "\n",
        "# Align the training and testing data, keep only columns present in both dataframes\n",
        "app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
        "\n",
        "# Add the target back in\n",
        "app_train['TARGET'] = train_labels\n",
        "\n",
        "print('Training Features shape: ', app_train.shape)\n",
        "print('Testing Features shape: ', app_test.shape)"
      ],
      "metadata": {
        "id": "F3r05FQ_KySr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(app_train['DAYS_BIRTH'] / -365).describe()"
      ],
      "metadata": {
        "id": "BXVULCgnLEAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train['DAYS_EMPLOYED'].describe()"
      ],
      "metadata": {
        "id": "10jpde4kLGQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
        "plt.xlabel('Days Employment');"
      ],
      "metadata": {
        "id": "uFjhUThFLIot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n",
        "non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n",
        "print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\n",
        "print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\n",
        "print('There are %d anomalous days of employment' % len(anom))"
      ],
      "metadata": {
        "id": "ViqAruwZLK_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an anomalous flag column\n",
        "app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n",
        "\n",
        "# Replace the anomalous values with nan\n",
        "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
        "\n",
        "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
        "plt.xlabel('Days Employment');"
      ],
      "metadata": {
        "id": "y9c2qb3sLNsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\n",
        "app_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n",
        "\n",
        "print('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))"
      ],
      "metadata": {
        "id": "dnJhqDbSLQpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find correlations with the target and sort\n",
        "correlations = app_train.corr()['TARGET'].sort_values()\n",
        "\n",
        "# Display correlations\n",
        "print('Most Positive Correlations:\\n', correlations.tail(15))\n",
        "print('\\nMost Negative Correlations:\\n', correlations.head(15))"
      ],
      "metadata": {
        "id": "54GKcrNcLTUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the correlation of the positive days since birth and target\n",
        "app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n",
        "app_train['DAYS_BIRTH'].corr(app_train['TARGET'])"
      ],
      "metadata": {
        "id": "IsHJZ8X8LVr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the style of plots\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Plot the distribution of ages in years\n",
        "plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\n",
        "plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');"
      ],
      "metadata": {
        "id": "v7vxAM3TLX7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 8))\n",
        "\n",
        "# KDE plot of loans that were repaid on time\n",
        "sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n",
        "\n",
        "# KDE plot of loans which were not repaid on time\n",
        "sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n",
        "\n",
        "# Labeling of plot\n",
        "plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');"
      ],
      "metadata": {
        "id": "qZkrnmSrLaUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Age information into a separate dataframe\n",
        "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
        "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
        "\n",
        "# Bin the age data\n",
        "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n",
        "age_data.head(10)"
      ],
      "metadata": {
        "id": "cny6g8zhLdFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by the bin and calculate averages\n",
        "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
        "age_groups"
      ],
      "metadata": {
        "id": "uOq7dhdfLfZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (8, 8))\n",
        "\n",
        "# Graph the age bins and the average of the target as a bar plot\n",
        "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
        "\n",
        "# Plot labeling\n",
        "plt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\n",
        "plt.title('Failure to Repay by Age Group');"
      ],
      "metadata": {
        "id": "IKWRddxYLjue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the EXT_SOURCE variables and show correlations\n",
        "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
        "ext_data_corrs = ext_data.corr()\n",
        "ext_data_corrs"
      ],
      "metadata": {
        "id": "VkvQWuxoLmW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (8, 6))\n",
        "\n",
        "# Heatmap of correlations\n",
        "sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
        "plt.title('Correlation Heatmap');"
      ],
      "metadata": {
        "id": "wFOEWDCWLow2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 12))\n",
        "\n",
        "# iterate through the sources\n",
        "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
        "\n",
        "    # create a new subplot for each source\n",
        "    plt.subplot(3, 1, i + 1)\n",
        "    # plot repaid loans\n",
        "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n",
        "    # plot loans that were not repaid\n",
        "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n",
        "\n",
        "    # Label the plots\n",
        "    plt.title('Distribution of %s by Target Value' % source)\n",
        "    plt.xlabel('%s' % source); plt.ylabel('Density');\n",
        "\n",
        "plt.tight_layout(h_pad = 2.5)\n"
      ],
      "metadata": {
        "id": "8h2OF6ulLrUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the data for plotting\n",
        "plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n",
        "\n",
        "# Add in the age of the client in years\n",
        "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
        "\n",
        "# Drop na values and limit to first 100000 rows\n",
        "plot_data = plot_data.dropna().loc[:100000, :]\n",
        "\n",
        "# Function to calculate correlation coefficient between two columns\n",
        "def corr_func(x, y, **kwargs):\n",
        "    r = np.corrcoef(x, y)[0][1]\n",
        "    ax = plt.gca()\n",
        "    ax.annotate(\"r = {:.2f}\".format(r),\n",
        "                xy=(.2, .8), xycoords=ax.transAxes,\n",
        "                size = 20)\n",
        "\n",
        "# Create the pairgrid object\n",
        "grid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n",
        "                    hue = 'TARGET',\n",
        "                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n",
        "\n",
        "# Upper is a scatter plot\n",
        "grid.map_upper(plt.scatter, alpha = 0.2)\n",
        "\n",
        "# Diagonal is a histogram\n",
        "grid.map_diag(sns.kdeplot)\n",
        "\n",
        "# Bottom is density plot\n",
        "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
        "\n",
        "plt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);"
      ],
      "metadata": {
        "id": "aTGojinRLumS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new dataframe for polynomial features\n",
        "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
        "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
        "\n",
        "# imputer for handling missing values\n",
        "from sklearn.preprocessing import Imputer\n",
        "imputer = Imputer(strategy = 'median')\n",
        "\n",
        "poly_target = poly_features['TARGET']\n",
        "\n",
        "poly_features = poly_features.drop(columns = ['TARGET'])\n",
        "\n",
        "# Need to impute missing values\n",
        "poly_features = imputer.fit_transform(poly_features)\n",
        "poly_features_test = imputer.transform(poly_features_test)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Create the polynomial object with specified degree\n",
        "poly_transformer = PolynomialFeatures(degree = 3)"
      ],
      "metadata": {
        "id": "3q38FvIhLx0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the polynomial features\n",
        "poly_transformer.fit(poly_features)\n",
        "\n",
        "# Transform the features\n",
        "poly_features = poly_transformer.transform(poly_features)\n",
        "poly_features_test = poly_transformer.transform(poly_features_test)\n",
        "print('Polynomial Features shape: ', poly_features.shape)"
      ],
      "metadata": {
        "id": "Iq_75COjL1un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]"
      ],
      "metadata": {
        "id": "_2izMrnIL5Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe of the features\n",
        "poly_features = pd.DataFrame(poly_features,\n",
        "                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
        "                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
        "\n",
        "# Add in the target\n",
        "poly_features['TARGET'] = poly_target\n",
        "\n",
        "# Find the correlations with the target\n",
        "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
        "\n",
        "# Display most negative and most positive\n",
        "print(poly_corrs.head(10))\n",
        "print(poly_corrs.tail(5))"
      ],
      "metadata": {
        "id": "XOODln7eL8-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put test features into dataframe\n",
        "poly_features_test = pd.DataFrame(poly_features_test,\n",
        "                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
        "                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
        "\n",
        "# Merge polynomial features into training dataframe\n",
        "poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
        "app_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Merge polnomial features into testing dataframe\n",
        "poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
        "app_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Align the dataframes\n",
        "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
        "\n",
        "# Print out the new shapes\n",
        "print('Training data with polynomial features shape: ', app_train_poly.shape)\n",
        "print('Testing data with polynomial features shape:  ', app_test_poly.shape)"
      ],
      "metadata": {
        "id": "vLfwQ-0DL_hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train_domain = app_train.copy()\n",
        "app_test_domain = app_test.copy()\n",
        "\n",
        "app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n",
        "app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n",
        "app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n",
        "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']"
      ],
      "metadata": {
        "id": "vHvoi7SIMC-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n",
        "app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n",
        "app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n",
        "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']"
      ],
      "metadata": {
        "id": "EGsamqg0MFc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12, 20))\n",
        "# iterate through the new features\n",
        "for i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n",
        "\n",
        "    # create a new subplot for each source\n",
        "    plt.subplot(4, 1, i + 1)\n",
        "    # plot repaid loans\n",
        "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n",
        "    # plot loans that were not repaid\n",
        "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n",
        "\n",
        "    # Label the plots\n",
        "    plt.title('Distribution of %s by Target Value' % feature)\n",
        "    plt.xlabel('%s' % feature); plt.ylabel('Density');\n",
        "\n",
        "plt.tight_layout(h_pad = 2.5)"
      ],
      "metadata": {
        "id": "2GWWiuPOMO8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
        "\n",
        "# Drop the target from the training data\n",
        "if 'TARGET' in app_train:\n",
        "    train = app_train.drop(columns = ['TARGET'])\n",
        "else:\n",
        "    train = app_train.copy()\n",
        "\n",
        "# Feature names\n",
        "features = list(train.columns)\n",
        "\n",
        "# Copy of the testing data\n",
        "test = app_test.copy()\n",
        "\n",
        "# Median imputation of missing values\n",
        "imputer = Imputer(strategy = 'median')\n",
        "\n",
        "# Scale each feature to 0-1\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "# Fit on the training data\n",
        "imputer.fit(train)\n",
        "\n",
        "# Transform both training and testing data\n",
        "train = imputer.transform(train)\n",
        "test = imputer.transform(app_test)\n",
        "\n",
        "# Repeat with the scaler\n",
        "scaler.fit(train)\n",
        "train = scaler.transform(train)\n",
        "test = scaler.transform(test)\n",
        "\n",
        "print('Training data shape: ', train.shape)\n",
        "print('Testing data shape: ', test.shape)"
      ],
      "metadata": {
        "id": "7r7Xep4iMSQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Make the model with the specified regularization parameter\n",
        "log_reg = LogisticRegression(C = 0.0001)\n",
        "\n",
        "# Train on the training data\n",
        "log_reg.fit(train, train_labels)"
      ],
      "metadata": {
        "id": "-iELqEMJMUtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "# Make sure to select the second column only\n",
        "log_reg_pred = log_reg.predict_proba(test)[:, 1]"
      ],
      "metadata": {
        "id": "DMEkdgDwMW1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = log_reg_pred\n",
        "\n",
        "submit.head()"
      ],
      "metadata": {
        "id": "UvlaH9weMYxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the submission to a csv file\n",
        "submit.to_csv('log_reg_baseline.csv', index = False)"
      ],
      "metadata": {
        "id": "lR-gYWU_Ma16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Make the random forest classifier\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)"
      ],
      "metadata": {
        "id": "cmbsDf14Mdem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on the training data\n",
        "random_forest.fit(train, train_labels)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importance_values = random_forest.feature_importances_\n",
        "feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest.predict_proba(test)[:, 1]"
      ],
      "metadata": {
        "id": "Y7y4oDimMt2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = predictions\n",
        "\n",
        "# Save the submission dataframe\n",
        "submit.to_csv('random_forest_baseline.csv', index = False)"
      ],
      "metadata": {
        "id": "acuAo9JKMwXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poly_features_names = list(app_train_poly.columns)\n",
        "\n",
        "# Impute the polynomial features\n",
        "imputer = Imputer(strategy = 'median')\n",
        "\n",
        "poly_features = imputer.fit_transform(app_train_poly)\n",
        "poly_features_test = imputer.transform(app_test_poly)\n",
        "\n",
        "# Scale the polynomial features\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "poly_features = scaler.fit_transform(poly_features)\n",
        "poly_features_test = scaler.transform(poly_features_test)\n",
        "\n",
        "random_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)"
      ],
      "metadata": {
        "id": "Y2iUC7jlMyZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on the training data\n",
        "random_forest_poly.fit(poly_features, train_labels)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]"
      ],
      "metadata": {
        "id": "Z2CtJuPpM0Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = predictions\n",
        "\n",
        "# Save the submission dataframe\n",
        "submit.to_csv('random_forest_baseline_engineered.csv', index = False)"
      ],
      "metadata": {
        "id": "YNXXU1gKM2Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train_domain = app_train_domain.drop(columns = 'TARGET')\n",
        "\n",
        "domain_features_names = list(app_train_domain.columns)\n",
        "\n",
        "# Impute the domainnomial features\n",
        "imputer = Imputer(strategy = 'median')\n",
        "\n",
        "domain_features = imputer.fit_transform(app_train_domain)\n",
        "domain_features_test = imputer.transform(app_test_domain)\n",
        "\n",
        "# Scale the domainnomial features\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "domain_features = scaler.fit_transform(domain_features)\n",
        "domain_features_test = scaler.transform(domain_features_test)\n",
        "\n",
        "random_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n",
        "\n",
        "# Train on the training data\n",
        "random_forest_domain.fit(domain_features, train_labels)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importance_values_domain = random_forest_domain.feature_importances_\n",
        "feature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]"
      ],
      "metadata": {
        "id": "5qWJvPVzM37-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = predictions\n",
        "\n",
        "# Save the submission dataframe\n",
        "submit.to_csv('random_forest_baseline_domain.csv', index = False)"
      ],
      "metadata": {
        "id": "OpGdEQI0M6At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_importances(df):\n",
        "    \"\"\"\n",
        "    Plot importances returned by a model. This can work with any measure of\n",
        "    feature importance provided that higher importance is better.\n",
        "\n",
        "    Args:\n",
        "        df (dataframe): feature importances. Must have the features in a column\n",
        "        called `features` and the importances in a column called `importance\n",
        "\n",
        "    Returns:\n",
        "        shows a plot of the 15 most importance features\n",
        "\n",
        "        df (dataframe): feature importances sorted by importance (highest to lowest)\n",
        "        with a column for normalized importance\n",
        "        \"\"\"\n",
        "\n",
        "    # Sort features according to importance\n",
        "    df = df.sort_values('importance', ascending = False).reset_index()\n",
        "\n",
        "    # Normalize the feature importances to add up to one\n",
        "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
        "\n",
        "    # Make a horizontal bar chart of feature importances\n",
        "    plt.figure(figsize = (10, 6))\n",
        "    ax = plt.subplot()\n",
        "\n",
        "    # Need to reverse the index to plot most important on top\n",
        "    ax.barh(list(reversed(list(df.index[:15]))),\n",
        "            df['importance_normalized'].head(15),\n",
        "            align = 'center', edgecolor = 'k')\n",
        "\n",
        "    # Set the yticks and labels\n",
        "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
        "    ax.set_yticklabels(df['feature'].head(15))\n",
        "\n",
        "    # Plot labeling\n",
        "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
        "    plt.show()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "yHUrx95DM9xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the feature importances for the default features\n",
        "feature_importances_sorted = plot_feature_importances(feature_importances)"
      ],
      "metadata": {
        "id": "WYlj-PA_NAL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)"
      ],
      "metadata": {
        "id": "7liuZrm1NCzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "\n",
        "def model(features, test_features, encoding = 'ohe', n_folds = 5):\n",
        "\n",
        "    \"\"\"Train and test a light gradient boosting model using\n",
        "    cross validation.\n",
        "\n",
        "    Parameters\n",
        "    --------\n",
        "        features (pd.DataFrame):\n",
        "            dataframe of training features to use\n",
        "            for training a model. Must include the TARGET column.\n",
        "        test_features (pd.DataFrame):\n",
        "            dataframe of testing features to use\n",
        "            for making predictions with the model.\n",
        "        encoding (str, default = 'ohe'):\n",
        "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
        "            n_folds (int, default = 5): number of folds to use for cross validation\n",
        "\n",
        "    Return\n",
        "    --------\n",
        "        submission (pd.DataFrame):\n",
        "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
        "            predicted by the model.\n",
        "        feature_importances (pd.DataFrame):\n",
        "            dataframe with the feature importances from the model.\n",
        "        valid_metrics (pd.DataFrame):\n",
        "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the ids\n",
        "    train_ids = features['SK_ID_CURR']\n",
        "    test_ids = test_features['SK_ID_CURR']\n",
        "\n",
        "    # Extract the labels for training\n",
        "    labels = features['TARGET']\n",
        "\n",
        "    # Remove the ids and target\n",
        "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
        "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
        "\n",
        "    # One Hot Encoding\n",
        "    if encoding == 'ohe':\n",
        "        features = pd.get_dummies(features)\n",
        "        test_features = pd.get_dummies(test_features)\n",
        "\n",
        "        # Align the dataframes by the columns\n",
        "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
        "\n",
        "        # No categorical indices to record\n",
        "        cat_indices = 'auto'\n",
        "\n",
        "    # Integer label encoding\n",
        "    elif encoding == 'le':\n",
        "\n",
        "        # Create a label encoder\n",
        "        label_encoder = LabelEncoder()\n",
        "\n",
        "        # List for storing categorical indices\n",
        "        cat_indices = []\n",
        "\n",
        "        # Iterate through each column\n",
        "        for i, col in enumerate(features):\n",
        "            if features[col].dtype == 'object':\n",
        "                # Map the categorical features to integers\n",
        "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
        "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
        "\n",
        "                # Record the categorical indices\n",
        "                cat_indices.append(i)\n",
        "\n",
        "    # Catch error if label encoding scheme is not valid\n",
        "    else:\n",
        "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
        "\n",
        "    print('Training Data Shape: ', features.shape)\n",
        "    print('Testing Data Shape: ', test_features.shape)\n",
        "\n",
        "    # Extract feature names\n",
        "    feature_names = list(features.columns)\n",
        "\n",
        "    # Convert to np arrays\n",
        "    features = np.array(features)\n",
        "    test_features = np.array(test_features)\n",
        "\n",
        "    # Create the kfold object\n",
        "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
        "\n",
        "    # Empty array for feature importances\n",
        "    feature_importance_values = np.zeros(len(feature_names))\n",
        "\n",
        "    # Empty array for test predictions\n",
        "    test_predictions = np.zeros(test_features.shape[0])\n",
        "\n",
        "    # Empty array for out of fold validation predictions\n",
        "    out_of_fold = np.zeros(features.shape[0])\n",
        "\n",
        "    # Lists for recording validation and training scores\n",
        "    valid_scores = []\n",
        "    train_scores = []\n",
        "\n",
        "    # Iterate through each fold\n",
        "    for train_indices, valid_indices in k_fold.split(features):\n",
        "\n",
        "        # Training data for the fold\n",
        "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
        "        # Validation data for the fold\n",
        "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
        "\n",
        "        # Create the model\n",
        "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary',\n",
        "                                   class_weight = 'balanced', learning_rate = 0.05,\n",
        "                                   reg_alpha = 0.1, reg_lambda = 0.1,\n",
        "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
        "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
        "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
        "                  early_stopping_rounds = 100, verbose = 200)\n",
        "\n",
        "        # Record the best iteration\n",
        "        best_iteration = model.best_iteration_\n",
        "\n",
        "        # Record the feature importances\n",
        "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
        "\n",
        "        # Make predictions\n",
        "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
        "\n",
        "        # Record the out of fold predictions\n",
        "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
        "\n",
        "        # Record the best score\n",
        "        valid_score = model.best_score_['valid']['auc']\n",
        "        train_score = model.best_score_['train']['auc']\n",
        "\n",
        "        valid_scores.append(valid_score)\n",
        "        train_scores.append(train_score)\n",
        "\n",
        "        # Clean up memory\n",
        "        gc.enable()\n",
        "        del model, train_features, valid_features\n",
        "        gc.collect()\n",
        "\n",
        "    # Make the submission dataframe\n",
        "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
        "\n",
        "    # Make the feature importance dataframe\n",
        "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
        "\n",
        "    # Overall validation score\n",
        "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
        "\n",
        "    # Add the overall scores to the metrics\n",
        "    valid_scores.append(valid_auc)\n",
        "    train_scores.append(np.mean(train_scores))\n",
        "\n",
        "    # Needed for creating dataframe of validation scores\n",
        "    fold_names = list(range(n_folds))\n",
        "    fold_names.append('overall')\n",
        "\n",
        "    # Dataframe of validation scores\n",
        "    metrics = pd.DataFrame({'fold': fold_names,\n",
        "                            'train': train_scores,\n",
        "                            'valid': valid_scores})\n",
        "\n",
        "    return submission, feature_importances, metrics"
      ],
      "metadata": {
        "id": "wlfkedpRNNMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission, fi, metrics = model(app_train, app_test)\n",
        "print('Baseline metrics')\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "fBX80DQRNQCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fi_sorted = plot_feature_importances(fi)"
      ],
      "metadata": {
        "id": "9bWFSd35NSdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('baseline_lgb.csv', index = False)"
      ],
      "metadata": {
        "id": "Fc0ZixvWNUkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train_domain['TARGET'] = train_labels\n",
        "\n",
        "# Test the domain knolwedge features\n",
        "submission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\n",
        "print('Baseline with domain knowledge features metrics')\n",
        "print(metrics_domain)"
      ],
      "metadata": {
        "id": "eQHY7ouJNWrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fi_sorted = plot_feature_importances(fi_domain)"
      ],
      "metadata": {
        "id": "XoBFd9aXNZPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_domain.to_csv('baseline_lgb_domain_features.csv', index = False)"
      ],
      "metadata": {
        "id": "2qae8n4QNgGy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}